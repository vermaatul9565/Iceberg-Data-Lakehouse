{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b741f421",
   "metadata": {},
   "source": [
    "## Spark for beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69579a0-a350-44a1-8051-36667bfa5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c75e8",
   "metadata": {},
   "source": [
    "#### Problem 1: Count word frequency from a text file (ignore case, strip punctuation).\n",
    "\n",
    "\n",
    "Shows RDD API: flatMap, map, reduceByKey.\n",
    "\n",
    "Avoid groupByKey for counting — reduceByKey is more efficient (combiner).\n",
    "\n",
    "Interview tip: Explain lazy evaluation and how collect() triggers computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb905a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('of', 1), ('world', 2), ('hello', 2), ('spark', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext \n",
    "\n",
    "lines = sc.parallelize([ \n",
    "    \"Hello world\",\n",
    "    \"Hello Spark\",\n",
    "    \"world of Spark\"\n",
    "]) \n",
    "\n",
    "def normalize(line): \n",
    "    return line.split(' ') # re.findall(r\"\\w+\", line.lower()) \n",
    "\n",
    "counts = (lines\n",
    "          .flatMap(lambda x: x.lower().split(' '))  # .flatMap(normalize)        # split into words \n",
    "          .map(lambda w: (w, 1))  # pair \n",
    "          .reduceByKey(lambda a,b: a+b)  # aggregate \n",
    "         )\n",
    "\n",
    "counts.collect()\n",
    "# for key, value in counts.collect(): \n",
    "#     print(key, value) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f80338-160d-478f-8d03-76b9c941d9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b58dcd6-21cc-47ac-90df-364f19d67b42",
   "metadata": {},
   "source": [
    "#### Problem 2: Given a CSV of user events (user_id, event_type, timestamp), compute number of click events per user.\n",
    "\n",
    "Use DataFrame API. Avoid collecting large result sets.\n",
    "\n",
    "If reading files, use spark.read.csv(..., header=True, schema=schema).\n",
    "\n",
    "Interview tip: Mention schema inference costs and why passing schema is better for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfc16fa0-ab7e-4cf3-8a84-ec4536f479d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+\n",
      "|user_id|event_type|          timestamp|\n",
      "+-------+----------+-------------------+\n",
      "|      1|     click|2025-09-30 12:00:00|\n",
      "|      1|      view|2025-09-30 12:00:10|\n",
      "|      2|     click|2025-09-30 12:01:00|\n",
      "|      1|     click|2025-09-30 12:02:00|\n",
      "+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.csv('spark-warehouse/user_event.csv', header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4260016-1614-461d-a217-e0b6e8017578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+\n",
      "|user_id|event_type|          timestamp|\n",
      "+-------+----------+-------------------+\n",
      "|      1|     click|2025-09-30 12:00:00|\n",
      "|      1|      view|2025-09-30 12:00:10|\n",
      "|      2|     click|2025-09-30 12:01:00|\n",
      "|      1|     click|2025-09-30 12:02:00|\n",
      "+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv('spark-warehouse/user_event.csv', schema = schema, sep=',', header=True, mode='FAILFAST')\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5fcb9a83-5201-464b-8910-417a4fec606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_type|\n",
      "+----------+\n",
      "|      view|\n",
      "|     click|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('event_type').distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5122dbcf-b8d0-46ce-ae86-6d8570fd4b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+\n",
      "|user_id|event_type|          timestamp|\n",
      "+-------+----------+-------------------+\n",
      "|      1|     click|2025-09-30 12:00:00|\n",
      "|      2|     click|2025-09-30 12:01:00|\n",
      "|      1|     click|2025-09-30 12:02:00|\n",
      "+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"event_type\")==\"click\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "77e6d9f0-3ad4-4c98-ab91-d7881c2d5640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|click_count_per_user|\n",
      "+-------+--------------------+\n",
      "|      1|                   2|\n",
      "|      2|                   1|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"event_type\")==\"click\") \\\n",
    ".groupBy(\"user_id\") \\\n",
    ".count() \\\n",
    ".withColumnRenamed(\"count\", \"click_count_per_user\") \\\n",
    ".show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b735e8-3abe-4a9b-bd85-cec3ff70775e",
   "metadata": {},
   "source": [
    "### 3. Top N per group using Window functions\n",
    "\n",
    "#### Problem: Given sales(product_id, category, amount), for each category return top 3 products by total sales.\n",
    "\n",
    "Use Window and row_number() (not rank() if you want strict top-N).\n",
    "\n",
    "If groups are huge, window can be expensive; consider pre-aggregating and using limit per partition techniques for scale.\n",
    "\n",
    "Interview tip: Explain difference between rank, dense_rank, row_number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3104f589-2279-43b1-a60d-d16f0b21faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n",
      "|product_id|category|amount|\n",
      "+----------+--------+------+\n",
      "|         1|       A|   100|\n",
      "|         2|       A|   200|\n",
      "|         3|       A|   150|\n",
      "|         4|       A|   130|\n",
      "|         5|       B|   300|\n",
      "|         6|       B|   100|\n",
      "+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "rows = [\n",
    "    (1,\"A\",100),(2,\"A\",200),(3,\"A\",150),(4,\"A\",130),\n",
    "    (5,\"B\",300),(6,\"B\",100)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(rows, schema=[\"product_id\", \"category\", \"amount\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "88856328-3a8e-4aa8-be4d-ce08c58d49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+\n",
      "|product_id|category|total_sales|\n",
      "+----------+--------+-----------+\n",
      "|         1|       A|        100|\n",
      "|         2|       A|        200|\n",
      "|         3|       A|        150|\n",
      "|         4|       A|        130|\n",
      "|         5|       B|        300|\n",
      "|         6|       B|        100|\n",
      "+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "df.groupBy(\"product_id\", \"category\").agg(_sum(\"amount\").alias(\"total_sales\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88e423-100b-4a2b-8c1b-ec0380645cdf",
   "metadata": {},
   "source": [
    "Using row_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "33ad3bc7-7812-4b3d-b840-d31bb5a63bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+\n",
      "|product_id|category|total_sales|\n",
      "+----------+--------+-----------+\n",
      "|         2|       A|        200|\n",
      "|         3|       A|        150|\n",
      "|         4|       A|        130|\n",
      "|         5|       B|        300|\n",
      "|         6|       B|        100|\n",
      "+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = df.groupBy(\"product_id\", \"category\").agg(_sum(\"amount\").alias(\"total_sales\"))\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "(\n",
    "    agg_df.withColumn(\"rn\", row_number().over(\n",
    "            Window.partitionBy(\"category\").orderBy(agg_df[\"total_sales\"].desc())\n",
    "        )\n",
    "    )\n",
    "    .filter(col(\"rn\")<=3)\n",
    "    .drop(\"rn\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45aae81-87b4-4f31-9ecf-9c6609bb00f0",
   "metadata": {},
   "source": [
    "Using dense_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a80c50e0-7275-48fc-8343-9aa8a7484990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+\n",
      "|product_id|category|total_sales|\n",
      "+----------+--------+-----------+\n",
      "|         2|       A|        200|\n",
      "|         3|       A|        150|\n",
      "|         4|       A|        130|\n",
      "|         5|       B|        300|\n",
      "|         6|       B|        100|\n",
      "+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = df.groupBy(\"product_id\", \"category\").agg(_sum(\"amount\").alias(\"total_sales\"))\n",
    "from pyspark.sql.functions import row_number, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "(\n",
    "    agg_df.withColumn(\"rn\", dense_rank().over(\n",
    "        Window.partitionBy(\"category\").orderBy(agg_df[\"total_sales\"].desc())\n",
    "    )\n",
    ")\n",
    ".filter(col(\"rn\")<=3)\n",
    ".drop(\"rn\")\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120a818-aa52-496f-854d-5862459166bc",
   "metadata": {},
   "source": [
    "### 4. Efficient join: broadcast join when one table is small\n",
    "\n",
    "#### Problem: Join orders (very large) with country_lookup (small) to add country_name to each order.\n",
    "\n",
    "Use broadcast() to avoid large shuffle when one side is small.\n",
    "\n",
    "Spark may auto-broadcast small datasets (config spark.sql.autoBroadcastJoinThreshold) — mention this.\n",
    "\n",
    "Interview tip: discuss memory tradeoffs: broadcasting increases driver/executor memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcb5ea-7fc5-454e-aa01-a4fcbd5e3ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
